Backend Technical Requirements & Architecture
Overview
This document outlines the complete technical specifications for the backend architecture of the CV Generation Dashboard. The system is designed to be scalable, secure, and AI-driven, handling complex tasks such as resume parsing, AI analysis, and custom domain management.

The implementation is divided into three integration volumes to ensure a phased, stable rollout.

Volume 1: Core Foundation & Dashboard Entities
Goal: Establish the secure foundation, database schemas, and robust CRUD operations for the main dashboard entities (Resumes, Portfolios) to replace all frontend mocks.

1.1 Technology Stack & Infrastructure
Framework: Python FastAPI (Recommended for AI proximity) or Node.js NestJS.
Database: PostgreSQL 16+ (Relational data, ACID compliance).
ORM: SQLAlchemy (Python) or Prisma (Node).
Object Storage: AWS S3 or MinIO (Self-hosted) for saving PDF exports and Profile Images.
Authentication: JWT-based stateless auth with Refresh Tokens. Data stored in users table.
1.2 Database Schema (Core)
The database must be normalized (3NF). Use UUIDv4 for all primary keys to allow easy migration/sharding.

Users Table users
id: UUID (PK)
email: VARCHAR(255) Unique, Indexed
password_hash: VARCHAR (Argon2id)
full_name: VARCHAR
avatar_url: VARCHAR (Nullable)
created_at: TIMESTAMP WITH TIME ZONE
last_login: TIMESTAMP WITH TIME ZONE
subscription_tier: ENUM('free', 'pro', 'enterprise') - Defaults to 'free'
Resumes Table resumes
id: UUID (PK)
user_id: UUID (FK -> users.id)
title: VARCHAR (e.g., "Software Engineer 2024")
content: JSONB
Why JSONB? Resume structures are effectively documents. Allows flexibility for new sections without schema migrations.
Schema Validation: Enforced by Pydantic/Zod at the API layer.
theme_config: JSONB (Visual settings: colors, fonts, layout_id, margins)
thumbnail_url: VARCHAR (Preview image generated by Puppeteer/Playwright)
is_public: BOOLEAN (Default: False)
slug: VARCHAR (Unique index, e.g., /r/milan-dev)
created_at: TIMESTAMP
updated_at: TIMESTAMP
Portfolios Table portfolios
id: UUID (PK)
user_id: UUID (FK -> users.id)
title: VARCHAR
subdomain: VARCHAR (Unique, e.g., milan.cvgen.app)
custom_domain: VARCHAR (Nullable, Unique, e.g., milan.com)
content: JSONB (Projects list, "About Me" markdown, Contact info)
theme_config: JSONB
is_published: BOOLEAN
created_at: TIMESTAMP
1.3 API Endpoints Specification (Core)
Standardized responses: { "data": ... } or { "error": { "code": "...", "message": "..." } }.

Authentication
POST /auth/register: Create account.
POST /auth/login: Returns access_token and refresh_token.
POST /auth/refresh: Rotate access tokens.
POST /auth/me: Get current user profile.
Resume Management
GET /resumes: List all resumes (pagination: limit, offset).
POST /resumes: Create new. Payload: { title: string, template_id: string }.
GET /resumes/:id: Fetch specific resume details.
PUT /resumes/:id: Update content/theme.
DELETE /resumes/:id: Soft delete (set deleted_at).
POST /resumes/:id/duplicate: Clone a resume.
File Storage Operations
POST /uploads/presigned-url: Generate S3 pre-signed URL for client-side uploads (Avatars, Project Screenshots).
Security: Use minimal scope and duration (e.g., 5 minutes).
Volume 2: AI Analysis Engine & Intelligence Layer
Goal: Implement the "Brain" of the operation. a robust AI architecture for parsing, analyzing, and scoring content.

2.1 AI Architecture & Pipeline
To prevent timeouts and ensure scalability, the AI layer must be asynchronous.

⚠️ Failed to render Mermaid diagram: Parse error on line 8
graph TD
    A[Client UI] -->|1. Request Analysis| B[API Gateway]
    B -->|2. Create Job| C[Redis Job Queue]
    B -->|3. Return JobID| A
    D[AI Worker 1] -->|4. Process Job| C
    D[AI Worker 2] -->|4. Process Job| C
    C -->|5. Fetch Resume Context| E[Database]
    D -->|6. Prompt LLM| F[LLM Gateway (OpenAI/Anthropic)]
    F -->|7. JSON Response| D
    D -->|8. Save Results| G[Analysis Results DB]
    D -->|9. Pub/Sub Update| A
2.2 Database Schema (AI Enhancements)
Analysis_Results Table analysis_results
id: UUID (PK)
target_id: UUID (Links to Resume or Portfolio FK)
target_type: ENUM('resume', 'portfolio')
overall_score: INTEGER (0-100)
status: ENUM('queued', 'processing', 'completed', 'failed')
metrics: JSONB
Example: { "ats_score": 90, "impact_score": 85, "brevity_score": 70, "grammar_score": 95 }
strengths: JSONB (Array of strings)
improvements: JSONB (Array of strings)
detailed_report: TEXT (Markdown formatted deep-dive generated by AI)
model_used: VARCHAR (e.g., "gpt-4-turbo")
created_at: TIMESTAMP
2.3 AI Logic & Prompt Engineering
The system requires a Prompt Manager to version-control prompts.

Pipeline A: Resume Analysis Scanner
Input: Full Resume JSON + Job Description (Optional). Tasks:

ATS Parser Simulation: Convert resume to plain text. Check if headers (Education, Experience) are detectable.
Impact Analyzer:
Identify "Weak Verbs" (e.g., "Responsible for", "Helped with").
Suggest "Power Verbs" (e.g., "Spearheaded", "Optimized", "Generated").
Check for "Quantifiable Results" (Numbers, %, $).
Typos & Grammar: Strict checking.
Pipeline B: Portfolio Reviewer
Input: Portfolio content + Project Descriptions. Tasks:

Project Depth: Analyze if case studies follow the STAR method (Situation, Task, Action, Result).
Tag Relevance: Check if skills tags match project descriptions.
2.4 API Endpoints (AI)
POST /ai/analyze: Trigger new analysis.
Body: { target_id: UUID, type: "resume" | "portfolio", job_description?: string }
Returns: { job_id: UUID, status: "queued" }
GET /ai/analyze/:job_id/status: Polling endpoint. Returns progress (0-100) and step.
GET /ai/history: Fetch saved analyses (Pagination supported).
GET /ai/reports/:id: Get full JSON report.
Volume 3: Domain Management & Public Infrastructure
Goal: Implement the complex logic for Custom Domains (White-labeling), SSL Auto-provisioning, and High-Performance Public Rendering.

3.1 Domain Management Logic
This requires integration with a Reverse Proxy provider or a programmable DNS/Proxy like Caddy.

Architecture:

Caddy / Nginx Gateway: Sits in front of the application.
On-Demand TLS: Automatically obtains Let's Encrypt certificates when a verified domain is accessed.
3.2 Database Schema (Domains)
Domains Table custom_domains
id: UUID (PK)
user_id: UUID (FK)
project_id: UUID (Link to Portfolio)
domain_name: VARCHAR (e.g., alex.design) - Normalized to lowercase.
verification_token: VARCHAR (Unique TXT value)
verification_status: ENUM('pending', 'verified', 'failed')
dns_checked_at: TIMESTAMP
ssl_status: ENUM('issuing', 'active', 'error')
record_type: ENUM('CNAME', 'A', 'NS')
created_at: TIMESTAMP
3.3 The Verification Workflow
Step-by-Step Implementation:

Initiation: User adds example.com. System generates verification_token (e.g., cvgen-verify=abc12345).
Instruction: System tells user to add:
Type: TXT, Host: _cvgen-verification, Value: cvgen-verify=abc12345
Type: CNAME, Host: www (or root), Value: hosting.cvgen.app
Verification Job (Background Worker):
Runs every 5 minutes for pending domains.
query: dig TXT _cvgen-verification.example.com
query: dig CNAME www.example.com
If expected values match -> Update status to verified.
Activation:
Once verified, the Proxy Manager (Caddy) allows traffic for example.com.
First request triggers SSL handshake and certificate generation (takes ~2s).
3.4 Public Serving Engine (The "Renderer")
The backend needs a specialized high-performance route for serving public portfolios.

Route: GET /view/:domain_or_slug Logic:

Check if Host header is a custom domain (e.g., alex.design).
If yes, lookup custom_domains table -> get project_id.
If Host is app domain, check path cvgen.app/p/:slug.
Lookup portfolios by slug.
SSR/ISR: Render the Vue application with the portfolio data injected.
Caching: Cache the rendered HTML in Redis for 60 seconds (Stale-while-revalidate pattern) to handle traffic spikes.
3.5 API Endpoints (Domains)
POST /domains: Add new domain.
POST /domains/:id/verify: Force manual verification check.
GET /domains: Status list with DNS instruction details.
DELETE /domains/:id: Release domain.
Integration Roadmap
Phase 1: The Foundation
Setup PostgreSQL & Redis.
Implement Users, Resumes, Portfolios tables.
Build Core API endpoints.
Connect Frontend "Resumes" and "Portfolios" pages to real API (Replace const resumes = ref([])).
Phase 2: Intelligence
Set up Python/Node AI Worker Service.
Implement Analysis_Results table.
Build queue system (BullMQ / Celery).
Connect "AI Insights" page 
startAnalysis
 function to backend POST /ai/analyze.
Phase 3: Global Reach
Implement Domains table and Verification Worker.
Set up Caddy/Nginx for wildcard domains.
Build the Public Renderer Service.
Connect "Domains/New" page to backend.